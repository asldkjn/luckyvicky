{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 불용어 리스트 정의 (한국어 불용어)\n",
    "with open(r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\stopwords-ko(한국어불용어).txt\", encoding='utf-8') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "\n",
    "# 추가 불용어 정의\n",
    "stopwords += ['좀', '것', '달바', '미스트', '스프레이', '1으로', '1이라', '1에', '너무', '느낌', '잘', '그냥', '많이', \n",
    "              '있어서', '같아요', '스프레이가', '미스트가', '미스트는', '약간의', '생각보다', '있으며', '살짝', '같아서', \n",
    "              '느낌이', '저는', '진짜', '엄청', '합니다', '더', '안', '느낌은', '효과는', '조금', '분들은', '크림미스트라서',\n",
    "              '크림미스트라', '크림', '크림이', '미스트라', '미스트라서', '그런지', '못', '이거', '완전', '아벤느', '다', '개', '걸',\n",
    "              '매우','음','확','전','제','달바는','막','넘','너무너무','특히','근데','취향이','정말','덜','뭔가',\n",
    "              '저의','굉장히','아니지만','아니고','해야할까요','제품','시',\n",
    "              '있는데','샀는데','아니라','기분','썼을것같은데','바꿔봤는데','그런','사봤는데','해야할까요','저에게는',\n",
    "              '하는데','아주','알겠더라구용','되게','나요','나서','나면서','진심','나니까','저한텐','저한테는','구매했어요',]\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_excel(r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\sorted_merged_dalba.xlsx\")\n",
    "data = data.where(pd.notna(data), None)  # 결측치 처리\n",
    "data = data.reset_index()\n",
    "\n",
    "# 인덱스 넘버가 3 + 7 * n인 경우 필터링하여 'Aspect2 Mapped Labels' 행 필터링\n",
    "filtered_data = data.iloc[[i for i in range(len(data)) if (i - 2) % 7 == 0]]\n",
    "\n",
    "# 키워드 설정\n",
    "aspect_keywords = [\"봄\"]\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    \"\"\"텍스트에서 특수문자 제거 및 불용어 필터링\"\"\"\n",
    "    cleaned_text = re.sub(r\"[^가-힣0-9\\s]\", \"\", text)\n",
    "    tokens = [word for word in cleaned_text.split() if word not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "# 네트워크 그래프 생성을 위한 데이터 수집\n",
    "nodes_data = set()  # 중복 방지를 위해 set 사용\n",
    "all_previous_tokens = []\n",
    "\n",
    "# 키워드와 관련된 단어 연결\n",
    "for idx, row in filtered_data.iterrows():\n",
    "    original_idx = data[data['index'] == row['index']].index[0]\n",
    "    tokenized_text_index = original_idx + 3  # 'Tokenized Text' 행\n",
    "    sentiment_row_index = tokenized_text_index - 5  # 감정 데이터는 동일 열의 -5행에 위치\n",
    "\n",
    "    if tokenized_text_index < len(data):\n",
    "\n",
    "        for col_index in range(2, len(data.columns)):  # 각 열을 순회\n",
    "            tokenized_text = str(data.iat[tokenized_text_index, col_index])\n",
    "\n",
    "            # 감정 데이터 가져오기\n",
    "            try:\n",
    "                sentiment_value = str(data.iat[sentiment_row_index, col_index]).strip()\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] 감정 데이터를 가져오는 중 에러 발생: {e}\")\n",
    "                sentiment_value = None\n",
    "\n",
    "            # Null 또는 빈 문자열 확인\n",
    "            if not tokenized_text or tokenized_text.strip() == \"\":\n",
    "                continue\n",
    "\n",
    "            # 키워드 포함 여부 확인\n",
    "            if not any(keyword in tokenized_text for keyword in aspect_keywords):\n",
    "                continue\n",
    "\n",
    "            # 키워드가 포함된 경우 처리\n",
    "            print(f\"[INFO] 키워드 포함 텍스트: {tokenized_text}, 감정: {sentiment_value}\")\n",
    "\n",
    "            # 기준 셀의 왼쪽과 오른쪽 셀 값을 추출\n",
    "            left_text = (\n",
    "                str(data.iat[tokenized_text_index, col_index - 1])\n",
    "                if col_index > 2  # 왼쪽 셀이 존재하는 경우\n",
    "                else None\n",
    "            )\n",
    "            right_text = (\n",
    "                str(data.iat[tokenized_text_index, col_index + 1])\n",
    "                if col_index < len(data.columns) - 1  # 오른쪽 셀이 존재하는 경우\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # 제외할 접미사를 리스트로 관리\n",
    "            EXCLUDED_SUFFIXES = (\"-\")\n",
    "            EXCLUDED_KEYWORDS = [\"-\"]\n",
    "            # 왼쪽 셀 추가\n",
    "            if left_text and left_text.strip():\n",
    "                stripped_left_text = left_text.strip()  # 공백 제거\n",
    "                # 접미사 검사\n",
    "                if any(stripped_left_text.endswith(suffix) for suffix in EXCLUDED_SUFFIXES):\n",
    "                    print(f\"[INFO] 왼쪽 셀이 '{EXCLUDED_SUFFIXES}' 중 하나로 끝나서 제외됨: {stripped_left_text}\")\n",
    "\n",
    "                elif any(keyword in stripped_left_text for keyword in EXCLUDED_KEYWORDS):\n",
    "                    print(f\"[INFO] 왼쪽 셀이 제외 키워드를 포함하여 제외됨: {stripped_left_text}\")\n",
    "                else:\n",
    "                    nodes_data.add((tokenized_text, stripped_left_text, tokenized_text_index, col_index))  # set에 추가\n",
    "                    all_previous_tokens.append(stripped_left_text)\n",
    "\n",
    "            # 오른쪽 셀 추가\n",
    "            if right_text and right_text.strip():\n",
    "                stripped_right_text = right_text.strip()  # 공백 제거\n",
    "                # 키워드 포함 여부 확인\n",
    "                if any(keyword in stripped_right_text for keyword in EXCLUDED_KEYWORDS):\n",
    "                    print(f\"[INFO] 오른쪽 셀이 제외 키워드를 포함하여 제외됨: {stripped_right_text}\")\n",
    "                else:\n",
    "                    nodes_data.add((tokenized_text, right_text.strip(),tokenized_text_index,col_index))  # set에 추가\n",
    "                    all_previous_tokens.append(right_text.strip())\n",
    "\n",
    " \n",
    "\n",
    "# 빈도수 카운터 생성\n",
    "if not all_previous_tokens:\n",
    "    print(\"[ERROR] 관련된 서술어가 없습니다. 전처리 또는 데이터 로드 로직을 확인하세요.\")\n",
    "else:\n",
    "    # 감정별로 빈도수를 따로 저장하기 위한 딕셔너리\n",
    "    sentiment_frequency = {\n",
    "        \"긍정\": Counter(),\n",
    "        \"부정\": Counter(),\n",
    "        \"중립\": Counter()\n",
    "    }\n",
    "\n",
    "    # 전체 빈도수 저장\n",
    "    total_frequency = Counter()\n",
    "\n",
    "    # 빈도수 계산\n",
    "    for tokenized_text, related_text, tokenized_text_index, col_index in nodes_data:\n",
    "        # 감정 값 가져오기\n",
    "        try:\n",
    "            sentiment_value = str(data.iat[tokenized_text_index - 5, col_index]).strip()\n",
    "            # B-긍정, I-긍정 같은 값을 \"긍정\"으로 통합\n",
    "            if sentiment_value.startswith(\"B-\") or sentiment_value.startswith(\"I-\"):\n",
    "                sentiment_value = sentiment_value[2:]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 감정 데이터를 가져오는 중 에러 발생: {e}\")\n",
    "            sentiment_value = None\n",
    "\n",
    "        # 유효한 감정 값인지 확인\n",
    "        if sentiment_value in sentiment_frequency:\n",
    "            sentiment_frequency[sentiment_value][related_text] += 1\n",
    "            total_frequency[related_text] += 1  # 전체 빈도수 업데이트\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크 그래프 생성\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\")\n",
    "net.toggle_physics(True)\n",
    "\n",
    "# 중심 노드와 연관 노드 간 연결 정보 기록\n",
    "connected_nodes = {}\n",
    "center_frequency = Counter()\n",
    "\n",
    "# 그래프 노드 및 엣지 추가\n",
    "EXCLUDED_MAIN_KEYWORDS = [\"-\"]\n",
    "\n",
    "for main_token, related_token, sentiment_row_index, col_index in nodes_data:\n",
    "    main_node_key = main_token.strip()  # 중심 노드 키 생성\n",
    "    print(f\"[DEBUG] Processing pair: main_token={main_token}, related_token={related_token}\")\n",
    "\n",
    "    # 유효성 검증 및 불용어 필터링\n",
    "    if not re.match(r\"^[가-힣]+$\", main_node_key) or not re.match(r\"^[가-힣]+$\", related_token):\n",
    "        print(f\"[INFO] main_token 또는 related_token이 유효하지 않음: {main_token}, {related_token}\")\n",
    "        continue\n",
    "    if main_token in stopwords or related_token in stopwords:\n",
    "        print(f\"[INFO] 불용어로 제외됨: {main_token} 또는 {related_token}\")\n",
    "        continue\n",
    "    if any(keyword in main_node_key for keyword in EXCLUDED_MAIN_KEYWORDS):\n",
    "        print(f\"[INFO] 중심 노드가 제외 키워드를 포함하여 추가되지 않음: {main_node_key}\")\n",
    "        continue\n",
    "\n",
    "    # 감정 값 가져오기\n",
    "    related_token_sentiment = None\n",
    "    try:\n",
    "        related_token_sentiment = str(data.iat[sentiment_row_index - 5, col_index]).strip()\n",
    "        if related_token_sentiment.startswith(\"B-\") or related_token_sentiment.startswith(\"I-\"):\n",
    "            related_token_sentiment = related_token_sentiment[2:]  # 통합\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] related_token 감정 데이터 가져오기 실패: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 연관 노드 추가\n",
    "    related_token_frequency = sentiment_frequency.get(related_token_sentiment, {}).get(related_token, 0)\n",
    "    if related_token_frequency >= 1:\n",
    "        if \"O\" in related_token_sentiment:\n",
    "            print(f\"[INFO] 연관 노드가 '중립' 또는 'O' 감정으로 추가되지 않음: {related_token}\")\n",
    "            continue\n",
    "\n",
    "        related_node_key = f\"{related_token}_{related_token_sentiment}\"\n",
    "        if related_node_key not in net.get_nodes():\n",
    "            node_size = min(15 + related_token_frequency * 2, 50)\n",
    "            related_token_color = {\n",
    "                \"긍정\": \"#6EC664\",\n",
    "                \"중립\": \"#BDC3C7\",\n",
    "                \"부정\": \"#E74C3C\"\n",
    "            }.get(related_token_sentiment, \"black\")\n",
    "            net.add_node(\n",
    "                related_node_key,\n",
    "                label=related_token,\n",
    "                color=related_token_color,\n",
    "                size=node_size,\n",
    "                title=f\"{related_token} (빈도: {related_token_frequency})\"\n",
    "            )\n",
    "            print(f\"[DEBUG] 추가된 연관 노드: {related_node_key}\")\n",
    "\n",
    "        # 엣지 및 중심 노드 추가\n",
    "        if main_node_key not in net.get_nodes():\n",
    "            net.add_node(\n",
    "                main_node_key,\n",
    "                label=main_token,\n",
    "                color={\"background\": \"white\", \"border\": \"black\"},\n",
    "                size=25,\n",
    "                title=f\"{main_token} (빈도: 0)\"  # 빈도수는 추후 갱신\n",
    "            )\n",
    "        # 엣지 추가\n",
    "        net.add_edge(main_node_key, related_node_key, color=related_token_color)\n",
    "        print(f\"[DEBUG] 엣지 추가됨: {main_node_key} -> {related_node_key}\")\n",
    "        connected_nodes.setdefault(main_node_key, []).append(related_node_key)\n",
    "\n",
    "# 중심 노드 빈도 계산 (화면에 표시된 연관 노드 기반)\n",
    "center_frequency.clear()  # 기존 빈도수 초기화\n",
    "for main_node_key, related_nodes in connected_nodes.items():\n",
    "    for related_node_key in related_nodes:\n",
    "        related_token = related_node_key.split(\"_\")[0]\n",
    "        related_token_sentiment = related_node_key.split(\"_\")[1]\n",
    "        related_token_frequency = sentiment_frequency.get(related_token_sentiment, {}).get(related_token, 0)\n",
    "        center_frequency[main_node_key] += related_token_frequency\n",
    "        print(f\"[DEBUG] 중심 노드 '{main_node_key}'에 '{related_token}'의 빈도 {related_token_frequency} 추가됨\")\n",
    "\n",
    "# 중심 노드 빈도 업데이트 (화면에 표시된 값으로 갱신)\n",
    "for node in net.nodes:\n",
    "    if node[\"id\"] in center_frequency:\n",
    "        node[\"title\"] = f\"{node['id']} (빈도: {center_frequency[node['id']]})\"\n",
    "        print(f\"[DEBUG] 중심 노드 '{node['id']}'의 빈도 갱신됨: {center_frequency[node['id']]}\")\n",
    "\n",
    "# 디버깅: 최종 중심 노드 빈도수 출력\n",
    "print(\"[DEBUG] 최종 중심 노드 빈도수:\")\n",
    "for node, freq in center_frequency.items():\n",
    "    print(f\"{node}: {freq}\")\n",
    "\n",
    "# 네트워크 그래프 시각화\n",
    "net.show(\"spring.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 네트워크에 표시된 모든 노드 데이터 가져오기\n",
    "all_nodes_data = []\n",
    "for node in net.nodes:\n",
    "    # 노드 정보 추출\n",
    "    node_id = node[\"id\"]\n",
    "    node_label = node.get(\"label\", \"\")\n",
    "    node_color = node.get(\"color\", \"\")\n",
    "    node_title = node.get(\"title\", \"\")  # 빈도수 정보 포함 가능\n",
    "\n",
    "    # 노드 종류 결정\n",
    "    if \"_\" in node_id:  # \"_\"이 포함된 경우 연관 노드\n",
    "        node_type = \"연관 노드\"\n",
    "    else:  # \"_\"이 없는 경우 중심 노드\n",
    "        node_type = \"중심 노드\"\n",
    "\n",
    "    # 감정 추출 (색상 기반)\n",
    "    if node_color == \"#6EC664\":  # 연한 녹색\n",
    "        node_sentiment = \"긍정\"\n",
    "    elif node_color == \"#BDC3C7\":  # 연한 회색 (중립)\n",
    "        node_sentiment = \"중립\"\n",
    "    elif node_color == \"#E74C3C\":  # 연한 빨간색 (부정)\n",
    "        node_sentiment = \"부정\"\n",
    "    else:\n",
    "        node_sentiment = \"무감정\"\n",
    "\n",
    "    # 빈도수 추출 (Title에서 추출)\n",
    "    try:\n",
    "        node_frequency = int(re.search(r\"\\(빈도: (\\d+)\\)\", node_title).group(1))\n",
    "    except AttributeError:\n",
    "        node_frequency = 0  # 빈도수 정보가 없으면 0으로 설정\n",
    "\n",
    "    # 데이터 추가\n",
    "    all_nodes_data.append({\n",
    "        \"Node ID\": node_id,\n",
    "        \"Node Type\": node_type,\n",
    "        \"Label\": node_label,\n",
    "        \"Sentiment\": node_sentiment,\n",
    "        \"Frequency\": node_frequency,\n",
    "        \"Source Nodes\": []  # 초기값으로 빈 리스트 설정 (연관 노드일 경우 추가적으로 설정)\n",
    "    })\n",
    "\n",
    "# DataFrame으로 변환\n",
    "all_nodes_df = pd.DataFrame(all_nodes_data)\n",
    "\n",
    "# 디버깅: 모든 노드 출력\n",
    "print(\"[DEBUG] 모든 노드 데이터:\")\n",
    "print(all_nodes_df.head(10))\n",
    "\n",
    "# 디버깅: 모든 엣지 데이터 출력\n",
    "print(\"[DEBUG] 모든 엣지 데이터:\")\n",
    "for edge in net.edges:\n",
    "    print(f\"Source: {edge['from']}, Target: {edge['to']}\")\n",
    "\n",
    "# 엣지 데이터를 활용하여 Source Nodes 설정\n",
    "for edge in net.edges:\n",
    "    source = edge[\"from\"]\n",
    "    target = edge[\"to\"]\n",
    "\n",
    "    # 디버깅: 현재 처리 중인 엣지\n",
    "    print(f\"[DEBUG] 처리 중인 엣지: Source={source}, Target={target}\")\n",
    "\n",
    "    # 타겟 노드가 연관 노드인 경우 Source Nodes에 소스 노드 추가\n",
    "    if \"_\" in target:  # 타겟 노드가 연관 노드\n",
    "        target_idx = all_nodes_df[all_nodes_df[\"Node ID\"] == target].index\n",
    "        if not target_idx.empty:\n",
    "            current_sources = all_nodes_df.at[target_idx[0], \"Source Nodes\"]\n",
    "            if isinstance(current_sources, list):  # 리스트 형태인 경우\n",
    "                current_sources.append(source)\n",
    "            else:  # 초기값이 없거나 비어 있을 경우\n",
    "                current_sources = [source]\n",
    "            # Source Nodes 갱신\n",
    "            all_nodes_df.at[target_idx[0], \"Source Nodes\"] = current_sources\n",
    "\n",
    "# Source Nodes를 문자열로 변환 (콤마로 연결)\n",
    "all_nodes_df[\"Source Nodes\"] = all_nodes_df[\"Source Nodes\"].apply(\n",
    "    lambda x: \", \".join(x) if isinstance(x, list) else \"\"\n",
    ")\n",
    "\n",
    "# 정렬: 중심 노드 기준, 빈도수 기준 내림차순\n",
    "all_nodes_df = all_nodes_df.sort_values(by=[\"Node Type\", \"Frequency\"], ascending=[True, False])\n",
    "\n",
    "# 결과 저장\n",
    "output_file = \"spring.xlsx\"\n",
    "all_nodes_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Processed node data saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
