{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV ##합치기\n",
    "## csv utf-8 형태로 변경 후 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 파일 동시에 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyn03\\AppData\\Local\\Temp\\ipykernel_29232\\597039110.py:16: DtypeWarning: Columns (212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row: 0 in file 다시\n",
      "Processing row: 100 in file 다시\n",
      "Processing row: 200 in file 다시\n",
      "Processing row: 300 in file 다시\n",
      "Processing row: 400 in file 다시\n",
      "Processing row: 500 in file 다시\n",
      "Processing row: 600 in file 다시\n",
      "Processing row: 700 in file 다시\n",
      "Processing row: 800 in file 다시\n",
      "Processing row: 900 in file 다시\n",
      "Processing row: 1000 in file 다시\n",
      "Processing row: 1100 in file 다시\n",
      "Processing row: 1200 in file 다시\n",
      "Processing row: 1300 in file 다시\n",
      "Processing row: 1400 in file 다시\n",
      "Processing row: 1500 in file 다시\n",
      "Processing row: 1600 in file 다시\n",
      "Processing row: 1700 in file 다시\n",
      "Processing row: 1800 in file 다시\n",
      "Processing row: 1900 in file 다시\n",
      "Processing row: 2000 in file 다시\n",
      "Processing row: 2100 in file 다시\n",
      "Processing row: 2200 in file 다시\n",
      "Processing row: 2300 in file 다시\n",
      "Processing row: 2400 in file 다시\n",
      "Processing row: 2500 in file 다시\n",
      "Processing row: 2600 in file 다시\n",
      "Processing row: 2700 in file 다시\n",
      "Processing row: 2800 in file 다시\n",
      "Processing row: 2900 in file 다시\n",
      "Processing row: 3000 in file 다시\n",
      "Processing row: 3100 in file 다시\n",
      "Processing row: 3200 in file 다시\n",
      "Processing row: 3300 in file 다시\n",
      "Processing row: 3400 in file 다시\n",
      "Processing row: 3500 in file 다시\n",
      "Processing row: 3600 in file 다시\n",
      "Processing row: 3700 in file 다시\n",
      "Processing row: 3800 in file 다시\n",
      "Processing row: 3900 in file 다시\n",
      "Processing row: 4000 in file 다시\n",
      "Processing row: 4100 in file 다시\n",
      "Processing row: 4200 in file 다시\n",
      "Processing row: 4300 in file 다시\n",
      "Processing row: 4400 in file 다시\n",
      "Processing row: 4500 in file 다시\n",
      "Processing row: 4600 in file 다시\n",
      "Processing row: 4700 in file 다시\n",
      "Processing row: 4800 in file 다시\n",
      "Processing row: 4900 in file 다시\n",
      "Processing row: 5000 in file 다시\n",
      "Processing row: 5100 in file 다시\n",
      "Processing row: 5200 in file 다시\n",
      "Processing row: 5300 in file 다시\n",
      "Processing row: 5400 in file 다시\n",
      "Processing row: 5500 in file 다시\n",
      "Processing row: 5600 in file 다시\n",
      "Processing row: 5700 in file 다시\n",
      "Processing row: 5800 in file 다시\n",
      "Processing row: 5900 in file 다시\n",
      "Processing row: 6000 in file 다시\n",
      "Processing row: 6100 in file 다시\n",
      "Processing row: 6200 in file 다시\n",
      "Processing row: 6300 in file 다시\n",
      "Processing row: 6400 in file 다시\n",
      "Processing row: 6500 in file 다시\n",
      "Processing row: 6600 in file 다시\n",
      "Processing row: 6700 in file 다시\n",
      "Processing row: 6800 in file 다시\n",
      "Processing row: 6900 in file 다시\n",
      "Processing row: 7000 in file 다시\n",
      "Processing row: 7100 in file 다시\n",
      "Processing row: 7200 in file 다시\n",
      "Processing row: 7300 in file 다시\n",
      "Processing row: 7400 in file 다시\n",
      "Processing row: 7500 in file 다시\n",
      "Processing row: 7600 in file 다시\n",
      "Processing row: 7700 in file 다시\n",
      "Processing row: 7800 in file 다시\n",
      "Processing row: 7900 in file 다시\n",
      "Processing row: 8000 in file 다시\n",
      "Processing row: 8100 in file 다시\n",
      "Processing row: 8200 in file 다시\n",
      "Processing row: 8300 in file 다시\n",
      "Processing row: 8400 in file 다시\n",
      "Processing row: 8500 in file 다시\n",
      "Processing row: 8600 in file 다시\n",
      "Processing row: 8700 in file 다시\n",
      "Processing row: 8800 in file 다시\n",
      "Processing row: 8900 in file 다시\n",
      "Processing row: 9000 in file 다시\n",
      "Processing row: 9100 in file 다시\n",
      "Processing row: 9200 in file 다시\n",
      "Processing row: 9300 in file 다시\n",
      "Processing row: 9400 in file 다시\n",
      "Processing row: 9500 in file 다시\n",
      "Processing row: 9600 in file 다시\n",
      "Processing row: 9700 in file 다시\n",
      "Processing row: 9800 in file 다시\n",
      "Processing row: 9900 in file 다시\n",
      "Processing row: 10000 in file 다시\n",
      "Processing row: 10100 in file 다시\n",
      "Processing row: 10200 in file 다시\n",
      "Processing row: 10300 in file 다시\n",
      "Processing row: 10400 in file 다시\n",
      "Processing row: 10500 in file 다시\n",
      "Processing row: 10600 in file 다시\n",
      "Processing row: 10700 in file 다시\n",
      "Processing row: 10800 in file 다시\n",
      "Processing row: 10900 in file 다시\n",
      "Processing row: 11000 in file 다시\n",
      "Processing row: 11100 in file 다시\n",
      "Processing row: 11200 in file 다시\n",
      "Processing row: 11300 in file 다시\n",
      "Processing row: 11400 in file 다시\n",
      "Processing row: 11500 in file 다시\n",
      "Processing row: 11600 in file 다시\n",
      "Processing row: 11700 in file 다시\n",
      "Processing row: 11800 in file 다시\n",
      "Processing row: 11900 in file 다시\n",
      "Processing row: 12000 in file 다시\n",
      "Processing row: 12100 in file 다시\n",
      "Processing row: 12200 in file 다시\n",
      "Processing row: 12300 in file 다시\n",
      "Processing row: 12400 in file 다시\n",
      "Processing row: 12500 in file 다시\n",
      "Processing row: 12600 in file 다시\n",
      "Processing row: 12700 in file 다시\n",
      "Processing row: 12800 in file 다시\n",
      "Processing row: 12900 in file 다시\n",
      "Processing row: 13000 in file 다시\n",
      "Processing row: 13100 in file 다시\n",
      "Processing row: 13200 in file 다시\n",
      "Processing row: 13300 in file 다시\n",
      "Processing row: 13400 in file 다시\n",
      "Processing row: 13500 in file 다시\n",
      "Processing row: 13600 in file 다시\n",
      "Processing row: 13700 in file 다시\n",
      "Processing row: 13800 in file 다시\n",
      "Processing row: 13900 in file 다시\n",
      "[INFO] 엑셀 파일 '\\Users\\kyn03\\Downloads\\11_12\\합치기\\다시.xlsx'로 저장 완료.\n",
      "[INFO] CSV 파일 '\\Users\\kyn03\\Downloads\\11_12\\합치기\\다시.csv'로 저장 완료.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# CSV 파일들이 있는 폴더 경로\n",
    "input_folder = r\"\\Users\\kyn03\\Downloads\\11_12\\분석\"\n",
    "output_folder = r\"\\Users\\kyn03\\Downloads\\11_12\\합치기\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 폴더의 모든 CSV 파일을 처리\n",
    "for file_path in glob(os.path.join(input_folder, \"*.csv\")):\n",
    "    original_file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    \n",
    "    # 첫 번째 열 저장\n",
    "    original_first_column = df.iloc[:, 0]\n",
    "\n",
    "    # 최종 결과 DataFrame 초기화\n",
    "    final_result = pd.DataFrame()\n",
    "\n",
    "    # 7줄씩 처리\n",
    "    for start in range(0, df.shape[0], 5):\n",
    "        chunk1 = df.iloc[start:start+5, 1:]\n",
    "        if start % 100 == 0:\n",
    "            print(f\"Processing row: {start} in file {original_file_name}\")\n",
    "\n",
    "        # 7번째 row(인덱스 6)를 sequence로 설정\n",
    "        if len(chunk1) < 5:\n",
    "            print(f\"Skipping incomplete chunk starting at row {start}\")\n",
    "            continue\n",
    "\n",
    "        seq_row = chunk1.iloc[4]\n",
    "\n",
    "        # 새로운 DataFrame 생성\n",
    "        result_df = pd.DataFrame()\n",
    "\n",
    "        # 6개의 row에서 데이터 결합\n",
    "        for i in range(4):\n",
    "            result_df = pd.concat([result_df, chunk1.iloc[[i]]], ignore_index=True)\n",
    "\n",
    "        # 빈 row 추가\n",
    "        result_df.loc[6] = [''] * result_df.shape[1]\n",
    "\n",
    "        col = 0\n",
    "        sharp_list = []\n",
    "        check = True\n",
    "\n",
    "        # 열 반복 처리\n",
    "        while col < seq_row.shape[0]:\n",
    "            token = str(seq_row.iloc[col])\n",
    "            now_col = col\n",
    "\n",
    "            # ##이 포함된 다음 열 병합\n",
    "            while col + 1 < seq_row.shape[0] and '##' in str(seq_row.iloc[col + 1]):\n",
    "                token += str(seq_row.iloc[col + 1][2:])\n",
    "                col += 1\n",
    "                sharp_list.append(col)\n",
    "                check = False\n",
    "\n",
    "            if check:\n",
    "                result_df.iloc[4, now_col] = token\n",
    "                col += 1\n",
    "            else:\n",
    "                result_df.iloc[4, now_col] = token\n",
    "                check = True\n",
    "\n",
    "        # sharp_list에 있는 column 삭제\n",
    "        result_df = result_df.drop(result_df.columns[sharp_list], axis=1)\n",
    "        current_columns = result_df.shape[1]\n",
    "\n",
    "        # 509개로 패딩\n",
    "        if current_columns < 509:\n",
    "            padding_df = pd.DataFrame(np.nan, index=result_df.index, columns=range(current_columns, 509))\n",
    "            result_df = pd.concat([result_df, padding_df], axis=1, ignore_index=True)\n",
    "\n",
    "        # 최종 결과에 추가\n",
    "        final_result = pd.concat([final_result, result_df], ignore_index=True)\n",
    "    \n",
    "    # 첫 번째 행 삭제 후 인덱스 재설정\n",
    "    final_result = final_result.drop(0).reset_index(drop=True)\n",
    "\n",
    "    # 원래 CSV 파일의 첫 번째 열을 최종 결과에 'Original First Column'이라는 이름으로 추가\n",
    "    final_result.insert(0, 'Original First Column', original_first_column.iloc[1:].values)\n",
    "\n",
    "    # 최종 결과 DataFrame 저장\n",
    "    output_file_excel = os.path.join(output_folder, f\"{original_file_name}.xlsx\")\n",
    "    final_result.to_excel(output_file_excel, index=False)\n",
    "\n",
    "    output_file_csv = os.path.join(output_folder, f\"{original_file_name}.csv\")\n",
    "    final_result.to_csv(output_file_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"[INFO] 엑셀 파일 '{output_file_excel}'로 저장 완료.\")\n",
    "    print(f\"[INFO] CSV 파일 '{output_file_csv}'로 저장 완료.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 달바"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyn03\\AppData\\Local\\Temp\\ipykernel_18712\\7891972.py:10: DtypeWarning: Columns (258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row: 0\n",
      "Processing row: 700\n",
      "Processing row: 1400\n",
      "Processing row: 2100\n",
      "Processing row: 2800\n",
      "Processing row: 3500\n",
      "Processing row: 4200\n",
      "Processing row: 4900\n",
      "Processing row: 5600\n",
      "Processing row: 6300\n",
      "Processing row: 7000\n",
      "Processing row: 7700\n",
      "Processing row: 8400\n",
      "Processing row: 9100\n",
      "Processing row: 9800\n",
      "[INFO] 엑셀 파일 '\\Users\\kyn03\\Downloads\\합치기\\리뷰어_3회이상.xlsx'로 저장 완료.\n",
      "[INFO] CSV 파일 '\\Users\\kyn03\\Downloads\\합치기\\리뷰어_3회이상.csv'로 저장 완료.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "file_path = r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\output\\분석\\리뷰어_3회이상.csv\"  # 경로 확인\n",
    "original_file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# 원래 CSV 파일의 첫 번째 열을 저장\n",
    "original_first_column = df.iloc[:, 0]  # 첫 번째 열 저장\n",
    "\n",
    "# 최종 결과 DataFrame 초기화\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "# 7줄씩 처리\n",
    "for start in range(0, df.shape[0], 7):\n",
    "    chunk1 = df.iloc[start:start+7, 1:]  # 첫 번째 열을 제외하고 나머지 열 선택\n",
    "    if start % 100 == 0:\n",
    "        print(f\"Processing row: {start}\")\n",
    "\n",
    "    # 7번째 row(인덱스 6)를 sequence로 설정\n",
    "    if len(chunk1) < 7:\n",
    "        print(f\"Skipping incomplete chunk starting at row {start}\")\n",
    "        continue\n",
    "\n",
    "    seq_row = chunk1.iloc[6]\n",
    "\n",
    "    # 새로운 DataFrame 생성\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # 6개의 row에서 데이터 결합\n",
    "    for i in range(6):\n",
    "        result_df = pd.concat([result_df, chunk1.iloc[[i]]], ignore_index=True)\n",
    "\n",
    "    # 빈 row 추가\n",
    "    result_df.loc[6] = [''] * result_df.shape[1]\n",
    "\n",
    "    col = 0\n",
    "    sharp_list = []\n",
    "    check = True\n",
    "\n",
    "    # 열 반복 처리\n",
    "    while col < seq_row.shape[0]:\n",
    "        token = str(seq_row.iloc[col])\n",
    "        now_col = col\n",
    "\n",
    "        # ##이 포함된 다음 열 병합\n",
    "        while col + 1 < seq_row.shape[0] and '##' in str(seq_row.iloc[col + 1]):\n",
    "            token += str(seq_row.iloc[col + 1][2:])\n",
    "            col += 1\n",
    "            sharp_list.append(col)\n",
    "            check = False\n",
    "\n",
    "        if check:\n",
    "            result_df.iloc[6, now_col] = token\n",
    "            col += 1\n",
    "        else:\n",
    "            result_df.iloc[6, now_col] = token\n",
    "            check = True\n",
    "\n",
    "    # sharp_list에 있는 column 삭제\n",
    "    result_df = result_df.drop(result_df.columns[sharp_list], axis=1)\n",
    "    current_columns = result_df.shape[1]\n",
    "\n",
    "    # 509개로 패딩\n",
    "    if current_columns < 509:\n",
    "        padding_df = pd.DataFrame(np.nan, index=result_df.index, columns=range(current_columns, 509))\n",
    "        result_df = pd.concat([result_df, padding_df], axis=1, ignore_index=True)\n",
    "\n",
    "    # 최종 결과에 추가\n",
    "    final_result = pd.concat([final_result, result_df], ignore_index=True)\n",
    "\n",
    "# 첫 번째 행 삭제 후 인덱스 재설정\n",
    "final_result = final_result.drop(0).reset_index(drop=True)\n",
    "\n",
    "# 원래 CSV 파일의 첫 번째 열을 최종 결과에 'Original First Column'이라는 이름으로 추가\n",
    "final_result.insert(0, 'Original First Column', original_first_column.iloc[1:].values)\n",
    "\n",
    "# 최종 결과 DataFrame 저장\n",
    "output_file_excel = rf\"\\Users\\kyn03\\Downloads\\합치기\\{original_file_name}.xlsx\"\n",
    "final_result.to_excel(output_file_excel, index=False)\n",
    "\n",
    "output_file_csv = rf\"\\Users\\kyn03\\Downloads\\합치기\\{original_file_name}.csv\"\n",
    "final_result.to_csv(output_file_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"[INFO] 엑셀 파일 '{output_file_excel}'로 저장 완료.\")\n",
    "print(f\"[INFO] CSV 파일 '{output_file_csv}'로 저장 완료.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 바이오힐보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyn03\\AppData\\Local\\Temp\\ipykernel_20044\\2086466860.py:8: DtypeWarning: Columns (140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row: 0\n",
      "Processing row: 700\n",
      "Processing row: 1400\n",
      "Processing row: 2100\n",
      "Processing row: 2800\n",
      "Processing row: 3500\n",
      "Processing row: 4200\n",
      "Processing row: 4900\n",
      "Processing row: 5600\n",
      "Processing row: 6300\n",
      "Processing row: 7000\n",
      "Processing row: 7700\n",
      "Processing row: 8400\n",
      "Processing row: 9100\n",
      "Processing row: 9800\n",
      "Processing row: 10500\n",
      "Processing row: 11200\n",
      "Processing row: 11900\n",
      "Processing row: 12600\n",
      "Processing row: 13300\n",
      "Processing row: 14000\n",
      "Processing row: 14700\n",
      "Processing row: 15400\n",
      "Processing row: 16100\n",
      "Processing row: 16800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Excel 파일 불러오기\n",
    "file_path = r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\sorted_boh.csv\"  # 경로 확인\n",
    "\n",
    "# Excel 파일 읽기\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# 최종 결과 DataFrame 초기화\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "# 7줄씩 처리\n",
    "for start in range(0, df.shape[0], 7):\n",
    "    chunk1 = df.iloc[start:start+7, 1:]  # 1번째 column부터 선택\n",
    "    if start % 100 == 0:\n",
    "        print(f\"Processing row: {start}\")\n",
    "\n",
    "    # 7번째 row(인덱스 6)를 sequence로 설정\n",
    "    if len(chunk1) < 7:\n",
    "        print(f\"Skipping incomplete chunk starting at row {start}\")\n",
    "        continue\n",
    "\n",
    "    seq_row = chunk1.iloc[6]\n",
    "\n",
    "    # 새로운 DataFrame 생성\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # 6개의 row에서 데이터 결합\n",
    "    for i in range(6):\n",
    "        result_df = pd.concat([result_df, chunk1.iloc[[i]]], ignore_index=True)\n",
    "\n",
    "    # 빈 row 추가\n",
    "    result_df.loc[6] = [''] * result_df.shape[1]\n",
    "\n",
    "    col = 0\n",
    "    sharp_list = []\n",
    "    check = True\n",
    "\n",
    "    # 열 반복 처리\n",
    "    while col < seq_row.shape[0]:\n",
    "        token = str(seq_row.iloc[col])\n",
    "        now_col = col\n",
    "\n",
    "        # ##이 포함된 다음 열 병합\n",
    "        while col + 1 < seq_row.shape[0] and '##' in str(seq_row.iloc[col + 1]):\n",
    "            token += str(seq_row.iloc[col + 1][2:])\n",
    "            col += 1\n",
    "            sharp_list.append(col)\n",
    "            check = False\n",
    "\n",
    "        if check:\n",
    "            result_df.iloc[6, now_col] = token\n",
    "            col += 1\n",
    "        else:\n",
    "            result_df.iloc[6, now_col] = token\n",
    "            check = True\n",
    "\n",
    "    # sharp_list에 있는 column 삭제\n",
    "    result_df = result_df.drop(result_df.columns[sharp_list], axis=1)\n",
    "    current_columns = result_df.shape[1]\n",
    "\n",
    "    # 509개로 패딩\n",
    "    if current_columns < 509:\n",
    "        padding_df = pd.DataFrame(np.nan, index=result_df.index, columns=range(current_columns, 509))\n",
    "        result_df = pd.concat([result_df, padding_df], axis=1, ignore_index=True)\n",
    "\n",
    "    # 최종 결과에 추가\n",
    "    final_result = pd.concat([final_result, result_df], ignore_index=True)\n",
    "\n",
    "# 최종 결과 DataFrame 저장\n",
    "final_result.to_excel(r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\sorted_merged_boh.xlsx\", index=False)\n",
    "final_result.to_csv(r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\sorted_merged_boh.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아벤느"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyn03\\AppData\\Local\\Temp\\ipykernel_20044\\3780617315.py:8: DtypeWarning: Columns (220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row: 0\n",
      "Processing row: 700\n",
      "Processing row: 1400\n",
      "Processing row: 2100\n",
      "Processing row: 2800\n",
      "Processing row: 3500\n",
      "Processing row: 4200\n",
      "Processing row: 4900\n",
      "Processing row: 5600\n",
      "Processing row: 6300\n",
      "Processing row: 7000\n",
      "Processing row: 7700\n",
      "Processing row: 8400\n",
      "Processing row: 9100\n",
      "Processing row: 9800\n",
      "Processing row: 10500\n",
      "Processing row: 11200\n",
      "Processing row: 11900\n",
      "Processing row: 12600\n",
      "Processing row: 13300\n",
      "Processing row: 14000\n",
      "Processing row: 14700\n",
      "Processing row: 15400\n",
      "Processing row: 16100\n",
      "Processing row: 16800\n",
      "Processing row: 17500\n",
      "Processing row: 18200\n",
      "Processing row: 18900\n",
      "Processing row: 19600\n",
      "Processing row: 20300\n",
      "Processing row: 21000\n",
      "Processing row: 21700\n",
      "Processing row: 22400\n",
      "Processing row: 23100\n",
      "Processing row: 23800\n",
      "Processing row: 24500\n",
      "Processing row: 25200\n",
      "Processing row: 25900\n",
      "Processing row: 26600\n",
      "Processing row: 27300\n",
      "Processing row: 28000\n",
      "Processing row: 28700\n",
      "Processing row: 29400\n",
      "Processing row: 30100\n",
      "Processing row: 30800\n",
      "Processing row: 31500\n",
      "Processing row: 32200\n",
      "Processing row: 32900\n",
      "Processing row: 33600\n",
      "Processing row: 34300\n",
      "Processing row: 35000\n",
      "Processing row: 35700\n",
      "Processing row: 36400\n",
      "Processing row: 37100\n",
      "Processing row: 37800\n",
      "Processing row: 38500\n",
      "Processing row: 39200\n",
      "Processing row: 39900\n",
      "Processing row: 40600\n",
      "Processing row: 41300\n",
      "Processing row: 42000\n",
      "Processing row: 42700\n",
      "Processing row: 43400\n",
      "Processing row: 44100\n",
      "Processing row: 44800\n",
      "Processing row: 45500\n",
      "Processing row: 46200\n",
      "Processing row: 46900\n",
      "Processing row: 47600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Excel 파일 불러오기\n",
    "file_path = r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\sorted_avene.csv\"  # 경로 확인\n",
    "\n",
    "# Excel 파일 읽기\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# 최종 결과 DataFrame 초기화\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "# 7줄씩 처리\n",
    "for start in range(0, df.shape[0], 7):\n",
    "    chunk1 = df.iloc[start:start+7, 1:]  # 1번째 column부터 선택\n",
    "    if start % 100 == 0:\n",
    "        print(f\"Processing row: {start}\")\n",
    "\n",
    "    # 7번째 row(인덱스 6)를 sequence로 설정\n",
    "    if len(chunk1) < 7:\n",
    "        print(f\"Skipping incomplete chunk starting at row {start}\")\n",
    "        continue\n",
    "\n",
    "    seq_row = chunk1.iloc[6]\n",
    "\n",
    "    # 새로운 DataFrame 생성\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # 6개의 row에서 데이터 결합\n",
    "    for i in range(6):\n",
    "        result_df = pd.concat([result_df, chunk1.iloc[[i]]], ignore_index=True)\n",
    "\n",
    "    # 빈 row 추가\n",
    "    result_df.loc[6] = [''] * result_df.shape[1]\n",
    "\n",
    "    col = 0\n",
    "    sharp_list = []\n",
    "    check = True\n",
    "\n",
    "    # 열 반복 처리\n",
    "    while col < seq_row.shape[0]:\n",
    "        token = str(seq_row.iloc[col])\n",
    "        now_col = col\n",
    "\n",
    "        # ##이 포함된 다음 열 병합\n",
    "        while col + 1 < seq_row.shape[0] and '##' in str(seq_row.iloc[col + 1]):\n",
    "            token += str(seq_row.iloc[col + 1][2:])\n",
    "            col += 1\n",
    "            sharp_list.append(col)\n",
    "            check = False\n",
    "\n",
    "        if check:\n",
    "            result_df.iloc[6, now_col] = token\n",
    "            col += 1\n",
    "        else:\n",
    "            result_df.iloc[6, now_col] = token\n",
    "            check = True\n",
    "\n",
    "    # sharp_list에 있는 column 삭제\n",
    "    result_df = result_df.drop(result_df.columns[sharp_list], axis=1)\n",
    "    current_columns = result_df.shape[1]\n",
    "\n",
    "    # 509개로 패딩\n",
    "    if current_columns < 509:\n",
    "        padding_df = pd.DataFrame(np.nan, index=result_df.index, columns=range(current_columns, 509))\n",
    "        result_df = pd.concat([result_df, padding_df], axis=1, ignore_index=True)\n",
    "\n",
    "    # 최종 결과에 추가\n",
    "    final_result = pd.concat([final_result, result_df], ignore_index=True)\n",
    "\n",
    "# 최종 결과 DataFrame 저장\n",
    "final_result.to_excel(r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\sorted_merged_avene.xlsx\", index=False)\n",
    "final_result.to_csv(r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\sorted_merged_avene.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
