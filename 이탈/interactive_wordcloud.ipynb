{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aspect_keywords = [\"사용감/착용감\", \"효과/성능/기능\", \"품질/디자인/구성\", \"편의성/활용성\", \"가격\", \"사이즈/무게/개수\", \"제조/유통/서비스\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json 파일 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 폴더 전체에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 파일의 속성별 단어 빈도수가 JSON 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# 불용어 리스트 파일에서 불러오기\n",
    "with open(r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\stopwords-ko(한국어불용어).txt\", encoding='utf-8') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "\n",
    "# 추가 불용어 정의\n",
    "stopwords += ['좀','것','달바','미스트','스프레이','1으로','1이라','1에','너무','느낌','잘','그냥','많이','있어서','같아요','스프레이가','미스트가','미스트는','약간의',\n",
    "              '생각보다','있으며','살짝','같아서','느낌이','저는','진짜','엄청','합니다', '더','안','느낌은','효과는','조금','분들은','크림미스트라서',\n",
    "              '크림미스트라','크림','크림이','미스트라','미스트라서','그런지','못','이거','완전','아벤느','다','수']\n",
    "\n",
    "# 입력 및 출력 폴더 경로 설정\n",
    "input_folder = r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\이탈\\합치기\"\n",
    "output_folder = r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\이탈\\워드클라우드_json\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 여러 속성 키워드 정의\n",
    "aspect_keywords = [\"사용감/착용감\", \"효과/성능/기능\", \"품질/디자인/구성\", \"편의성/활용성\", \"가격\", \"사이즈/무게/개수\", \"제조/유통/서비스\"]\n",
    "\n",
    "def preprocess_text(text_list):\n",
    "    cleaned_text = [re.sub(r\"[^가-힣0-9\\+\\s]\", \"\", text) for text in text_list]\n",
    "    cleaned_text = [text for text in cleaned_text if text.strip()]\n",
    "    return cleaned_text\n",
    "\n",
    "def save_word_frequencies_to_json(word_freq, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(word_freq, f, ensure_ascii=False)\n",
    "\n",
    "# 모든 엑셀 파일에 대해 반복 처리\n",
    "for file_path in glob(os.path.join(input_folder, \"*.xlsx\")):\n",
    "    original_file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "\n",
    "    # 데이터 로드\n",
    "    data = pd.read_excel(file_path)\n",
    "    data = data.where(pd.notna(data), None).reset_index()\n",
    "    data['index'] = data['index'].astype(str)\n",
    "\n",
    "    # 각 속성 키워드에 대해 반복하여 JSON 파일 생성\n",
    "    for aspect_keyword in aspect_keywords:\n",
    "        sanitized_keyword = aspect_keyword.replace(\"/\", \"_\")\n",
    "\n",
    "        filtered_data = data.iloc[[i for i in range(len(data)) if (i - 2) % 5 == 0]]\n",
    "        if filtered_data.apply(lambda row: row.astype(str).str.contains(aspect_keyword, na=False).any(), axis=1).any():\n",
    "            aspect_row_data = filtered_data[filtered_data.apply(lambda row: row.astype(str).str.contains(aspect_keyword, na=False).any(), axis=1)]\n",
    "\n",
    "            positive_b_words = []\n",
    "            negative_b_words = []\n",
    "            positive_i_words = []\n",
    "            negative_i_words = []\n",
    "\n",
    "            for idx, row in aspect_row_data.iterrows():\n",
    "                original_idx = data[data['index'] == row['index']].index[0]\n",
    "                sentiment_row_index = original_idx - 2\n",
    "                tokenized_text_index = original_idx + 1\n",
    "\n",
    "                if tokenized_text_index < len(data):\n",
    "                    for col in data.columns[1:]:\n",
    "                        if aspect_keyword in str(data.at[original_idx, col]) and pd.notna(data.at[sentiment_row_index, col]) and pd.notna(data.at[tokenized_text_index, col]):\n",
    "                            sentiment_value = data.at[sentiment_row_index, col]\n",
    "                            tokenized_text = str(data.at[tokenized_text_index, col])\n",
    "\n",
    "                            words = [word for word in tokenized_text.split() if not word.startswith('#') and word not in stopwords]\n",
    "                            cleaned_words = preprocess_text(words)\n",
    "\n",
    "                            if cleaned_words:\n",
    "                                if 'B-긍정' in sentiment_value:\n",
    "                                    positive_b_words.extend(cleaned_words)\n",
    "                                elif 'B-부정' in sentiment_value:\n",
    "                                    negative_b_words.extend(cleaned_words)\n",
    "                                elif 'I-긍정' in sentiment_value:\n",
    "                                    positive_i_words.extend(cleaned_words)\n",
    "                                elif 'I-부정' in sentiment_value:\n",
    "                                    negative_i_words.extend(cleaned_words)\n",
    "\n",
    "            positive_b_words_freq = Counter(positive_b_words)\n",
    "            negative_b_words_freq = Counter(negative_b_words)\n",
    "            positive_i_words_freq = Counter(positive_i_words)\n",
    "            negative_i_words_freq = Counter(negative_i_words)\n",
    "\n",
    "            save_word_frequencies_to_json(positive_b_words_freq, os.path.join(output_folder, f'{original_file_name}_{sanitized_keyword}_positive_b_words.json'))\n",
    "            save_word_frequencies_to_json(positive_i_words_freq, os.path.join(output_folder, f'{original_file_name}_{sanitized_keyword}_positive_i_words.json'))\n",
    "            save_word_frequencies_to_json(negative_b_words_freq, os.path.join(output_folder, f'{original_file_name}_{sanitized_keyword}_negative_b_words.json'))\n",
    "            save_word_frequencies_to_json(negative_i_words_freq, os.path.join(output_folder, f'{original_file_name}_{sanitized_keyword}_negative_i_words.json'))\n",
    "\n",
    "print(\"모든 파일의 속성별 단어 빈도수가 JSON 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 속성의 단어 빈도수가 JSON 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 불용어 리스트 파일에서 불러오기\n",
    "with open(r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\stopwords-ko(한국어불용어).txt\", encoding='utf-8') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "\n",
    "# 추가 불용어 정의\n",
    "stopwords += ['좀','것','달바','미스트','스프레이','1으로','1이라','1에','너무','느낌','잘','그냥','많이','있어서','같아요','스프레이가','미스트가','미스트는','약간의',\n",
    "              '생각보다','있으며','살짝','같아서','느낌이','저는','진짜','엄청','합니다', '더','안','느낌은','효과는','조금','분들은','크림미스트라서',\n",
    "              '크림미스트라','크림','크림이','미스트라','미스트라서','그런지','못','이거','완전','아벤느','다','수']\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = r\"\\Users\\kyn03\\OneDrive\\바탕 화면\\project_file\\sorted_merged_boh.xlsx\"\n",
    "if os.path.splitext(os.path.basename(file_path))[0] == \"sorted_merged_boh\":\n",
    "    original_file_name = \"바이오힐보\"\n",
    "elif os.path.splitext(os.path.basename(file_path))[0] == \"sorted_merged_dalba\":\n",
    "    original_file_name = \"달바\"\n",
    "else:\n",
    "    original_file_name = \"아벤느\"\n",
    "\n",
    "data = pd.read_excel(file_path)\n",
    "data = data.where(pd.notna(data), None)  # 중복된 행은 유지하되, 결측치를 None으로 채움\n",
    "data = data.reset_index()\n",
    "data['index'] = data['index'].astype(str)\n",
    "\n",
    "# 여러 속성 키워드 정의\n",
    "aspect_keywords = [\"사용감/착용감\", \"효과/성능/기능\", \"품질/디자인/구성\", \"편의성/활용성\", \"가격\", \"사이즈/무게/개수\", \"제조/유통/서비스\"]\n",
    "\n",
    "def preprocess_text(text_list):\n",
    "    # 한글, 숫자, '+' 기호, 공백만 남기고 모든 특수문자 제거\n",
    "    cleaned_text = [re.sub(r\"[^가-힣0-9\\+\\s]\", \"\", text) for text in text_list]\n",
    "    # 공백 제거\n",
    "    cleaned_text = [text for text in cleaned_text if text.strip()]\n",
    "    return cleaned_text\n",
    "\n",
    "# JSON 파일로 저장하는 함수\n",
    "def save_word_frequencies_to_json(word_freq, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(word_freq, f, ensure_ascii=False)\n",
    "\n",
    "# 각 속성 키워드에 대해 반복하여 JSON 파일 생성\n",
    "for aspect_keyword in aspect_keywords:\n",
    "    sanitized_keyword = aspect_keyword.replace(\"/\", \"_\")  # 파일 이름에 사용할 수 있도록 '/'을 '_'로 대체\n",
    "\n",
    "    # 'Aspect2 Mapped Labels' 행 필터링 및 특정 키워드 포함 여부 확인\n",
    "    filtered_data = data.iloc[[i for i in range(len(data)) if (i - 2) % 7 == 0]]\n",
    "    if filtered_data.apply(lambda row: row.astype(str).str.contains(aspect_keyword, na=False).any(), axis=1).any():\n",
    "        aspect_row_data = filtered_data[filtered_data.apply(lambda row: row.astype(str).str.contains(aspect_keyword, na=False).any(), axis=1)]\n",
    "\n",
    "        # 긍정/부정 감정 및 토큰화된 텍스트 추출\n",
    "        positive_b_words = []\n",
    "        negative_b_words = []\n",
    "        positive_i_words = []\n",
    "        negative_i_words = []\n",
    "\n",
    "        for idx, row in aspect_row_data.iterrows():\n",
    "            original_idx = data[data['index'] == row['index']].index[0]\n",
    "            sentiment_row_index = original_idx - 2\n",
    "            tokenized_text_index = original_idx + 3\n",
    "\n",
    "            if tokenized_text_index < len(data):\n",
    "                for col in data.columns[1:]:\n",
    "                    # None 체크 추가\n",
    "                    if aspect_keyword in str(data.at[original_idx, col]) and pd.notna(data.at[sentiment_row_index, col]) and pd.notna(data.at[tokenized_text_index, col]):\n",
    "                        sentiment_value = data.at[sentiment_row_index, col]\n",
    "                        tokenized_text = str(data.at[tokenized_text_index, col])\n",
    "\n",
    "                        # 불용어 제거 및 '#'으로 시작하는 단어 제거 후 전처리 적용\n",
    "                        words = [word for word in tokenized_text.split() if not word.startswith('#') and word not in stopwords]\n",
    "                        cleaned_words = preprocess_text(words)  # 전처리 함수 호출\n",
    "\n",
    "                        if cleaned_words:\n",
    "                            # 감정에 따라 단어 리스트에 추가\n",
    "                            if 'B-긍정' in sentiment_value:\n",
    "                                positive_b_words.extend(cleaned_words)\n",
    "                            elif 'B-부정' in sentiment_value:\n",
    "                                negative_b_words.extend(cleaned_words)\n",
    "                            elif 'I-긍정' in sentiment_value:\n",
    "                                positive_i_words.extend(cleaned_words)\n",
    "                            elif 'I-부정' in sentiment_value:\n",
    "                                negative_i_words.extend(cleaned_words)\n",
    "\n",
    "        # 각 감정별 단어 빈도수 계산\n",
    "        positive_b_words_freq = Counter(positive_b_words)\n",
    "        negative_b_words_freq = Counter(negative_b_words)\n",
    "        positive_i_words_freq = Counter(positive_i_words)\n",
    "        negative_i_words_freq = Counter(negative_i_words)\n",
    "\n",
    "        # 각 감정별 JSON 파일로 저장\n",
    "        save_word_frequencies_to_json(positive_b_words_freq, f'{original_file_name}_{sanitized_keyword}_positive_b_words.json')\n",
    "        save_word_frequencies_to_json(positive_i_words_freq, f'{original_file_name}_{sanitized_keyword}_positive_i_words.json')\n",
    "        save_word_frequencies_to_json(negative_b_words_freq, f'{original_file_name}_{sanitized_keyword}_negative_b_words.json')\n",
    "        save_word_frequencies_to_json(negative_i_words_freq, f'{original_file_name}_{sanitized_keyword}_negative_i_words.json')\n",
    "\n",
    "print(\"모든 속성의 단어 빈도수가 JSON 파일로 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
